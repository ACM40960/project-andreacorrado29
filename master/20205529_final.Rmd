---
title: "South Africa drought and wildlife survival"
author: "Andrea Corrado 20205529"
bibliography: sample.bib
biblio-style: apalike
natbiboptions: round
header-includes:
  - \usepackage[hypcap=true]{caption}
  - \usepackage{amsmath}
output: bookdown::pdf_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, fig.align = 'center', dpi = 600)

rm(list = ls()) # clean env
gc() # clean memory

source('functions.R') # load useful functions  
source('pacakges_install.R') # install libraries if not available

# load useful libraries
library(ggplot2) 
library(reshape2) 
library(deSolve) 
library(minpack.lm)

```


\tableofcontents
\clearpage
<!-- citation example: \cite{first_line_after_bracket} -->
\twocolumn

<!-- ------------------------------------------------------------------------------------------------------------ -->
\section*{Abstract}










<!-- ------------------------------------------------------------------------------------------------------------ -->
\section{Introduction}

<!-- intro to the topic -->
During last decades, interest and awareness about the climate change has steeply increased. There have been several studies examining and reporting the potential consequences of these phenomenon. For instance, the temperature and rainfall change has been of particular interest. Since last century, we have witnessed a increasing frequency in the number and severity of droughts in particular territories such as South Africa \textit{SA} \cite{SA_drought_decades}. The consequences here can be very dramatic. Due to the fact that area such as \textit{SA} suffer from low economic power, a drought may drive towards dramatic consequences It can affect either agriculture, for a period that lasts up to 6 months, either hydrological up to 24 months \cite{SA_drought_decades}. 

<!-- studies on the topic -->
There have been several studies showing how to reduce the impact of drought on the agriculture field by growing restraint to higher temperature and computational model to predict droughts \cite{drought_asgriculture}.

<!-- shortly talk about agriculture conseuquences -->


<!-- the data -->
However, in this paper, we wish to focus our attention on the drought consequences on wildlife population evolution. For instance, different studies have shown as animals mortality grows during the highest temperature period within the driest areas \cite{drought_wildlife_mortality_1990}. We will expand this idea to more recent data about waterbirds \cite{waterbirds_data}. This data set will provide the number of individual for different species in each year within the last 45 years. By doing so, we are able to track the evolution of the species during time. However, the data set will not provide the number of new death, new born and the relative causes. It will be our job to build a model which allows us to investigate the relation other factors. In particular the data we are going to analyse regard the global temperature and rain fall amount \cite{rain_temp_data} from 1901 up to 2020 which would give a foundation to build on the model of interest.

<!-- our goal -->
Having access to this information, our goal is to build a model which relates the climate change to the number of individual for each species and predict how, following the current trend, these would evolve over time and the potential consequences these could drive towards.


In section \ref{sec:techback} we give a brief introduction to the methodology used within this research project. In section \ref{sec:eda} we present the data with an exploratory data analysis. In section \ref{sec:climate} we model the climate data and provide prediction on a possible future scenario. In section \ref{sec:population} we model the species count in relation to the climate data and provide prediction about the future number of individuals. In section \ref{sec:conclusion} we discuss the finding of the project.


<!-- the methods to be used -->
\section{Technical Background}
\label{sec:techback}

\subsection{Generalized Lienar Model}

<!-- statistics generalized linear model -->
In order to achieve the goal, we are going to employ different methodology which would allow us to model the data of interest. In particular, first of all we will related the temperature data to the rain fall amount. In order to do so, we will employ the Generalized Linear Model \cite{glm_intro} which theory would allow to establish a linear relationship between the quantity of interest (rain) and a set of covariates (temperature) plush a stochastic term which models the uncertainty about the random variable realisation

$$g(\mathbf{Y})  = \mathbf{X \beta} + \mathbf{\epsilon} $$
where we define a distribution $\mathcal{D}$ of interest on the stochastic term $\epsilon$
$$ \epsilon \sim \mathcal{D}(\theta)$$
and a link function $g(.)$ which is distribution dependent. 

The model will be validate by a set of diagnostic on the residuals $\mathbf{Y} - \mathbf{\hat{Y}}$ which provide meaningful insight on the goodness of fitness.

\subsection{Generalized Additive Model}

We will further extend the \textit{GLM} \cite{glm_intro} theory to allow for non-linear relationship between the predictors $\mathbf{X}$ and the response $\mathbf{Y}$ exploiting the \textit{Generalized Additive Model} theory \cite{gam_intro} while maintining the model linear in the parameters

$$g(\mathbf{Y})  = f(\mathbf{X}) + \mathbf{\epsilon} $$
where we define a distribution $\mathcal{D}$ of interest on the stochastic term $\epsilon$
$$ \epsilon \sim \mathcal{D}(\theta)$$
and a link function $g(.)$ which is distribution dependent. Moreover, we will define a function $f(.)$ which usually is a non linear function such as a \textit{spline} \cite{spline_intro}.


<!-- weighted least square -->
\subsection{Weighted Least Square}
During the research project, we perform different model estimation and to remedy to inadequate diagnostics, such as those violating the homoskedasticity assumption, we will employ \textit{Weighted Least Square} estimation theory \cite{wls_intro} which allows us to iteratively estimate a weight matrix $\mathbf{W}$ to be employed in the model estimation as a measure of the importance for each unit $\mathbf{x}_i\ i = \{1, \dots, N\}$ where $N$ is the total number of observations.

<!-- non parametric estiamtion: lowess -->
\subsection{Local Regression}



<!-- ode  -->
\subsection{Ordinary Differential Equation}
To track the evolution of a phenomenon over time, it is natural to think about \textit{Oridnary Differential Equation (ODE)} theory \cite{ode} which allows us to keep track of the change of a quantity in continuous time $$ \frac{d}{dt} x = f(x) $$ given initial condition $f(x, t = 0) = x_0$

<!-- LM algorithm for parameters finding -->
\subsection{Levenberg-Marquardt algorithm}
Very often, the function defined within an ODE is characterized by a set of parameters $\mathbf{\beta}$. In empirical studies,we often have access to data realization $\{(y_i, \mathbf{x}_i, )\}_{i=1}^n$ we want to find the parameters $\mathbf{\beta}$ so that $y = f(\mathbf{x}, \mathbf{\beta})$ best fits the data of interest.  The \textit{Levenberg-Marquardt} \cite{lm_algo} provide a useful and efficient solution to the problem by making smart combination of \textit{Gauss-Newton} and \textit{Gradien Descend} theory.

<!-- sensitivity analysis for model validation -->
\subsection{Sensitivity Analysis}
Mathematical modeling is often subject to very strong assumption which may be difficult to evaluate. In order to provide a robust framework, it is necessary to validate the model under perturbations on the findings. This is why, we will exploit \textit{Local Sensitivity Analysis} theory \cite{lsa_intro} to evaluate the robustness of the estimated models.


<!-- ------------------------------------------------------------------------------------------------------------ -->
\section{Exploratory Data Analysis}
\label{sec:eda}

\subsection{Rainfall amount}

Within this exploratory data analysis context, we give a short introduction to the data by illustrating some characteristics. In the first place, we investigate the rainfall data set. We have access to the information between January 1901 up to December 2020. For simplicity, we decide to compute an annual average and inspect the resulting distribution.

\begin{figure}
```{r esa_rain_preview}
rain <- read.csv('../data/pr_1901_2020_ZAF.csv') # load rain data
names(rain)[1] <- 'rain' # assign name to data set
rain_mean <- tapply(rain$rain, rain$Year, mean) # compute annual mean
rain_sd <- tapply(rain$rain, rain$Year, sd) # compute annual standard deviation
# collect data in a easy to handle data set
rain <- data.frame(year = as.numeric(names(rain_mean)), rain = rain_mean)

# plot histogram
hist(rain$rain, xlab = 'rain', main = 'Annual rain distribution', 
     col = 'grey90', border = 'white', breaks = 10)
```
\caption{Yearly rainfall distribution}
\label{fig:esa_rain_hist}
\end{figure}

From figure \ref{fig:esa_rain_hist} we can observe that the data distribution is somewhat bell-shaped. However, we can easily notice that the distribution is right skewed.  Clearly, the phenomenon is left bounded by 0 $x \in \mathbb{R}_+$ and we can rarely observe very extreme event such as very heavy precipitation. We are able to quantify the amount of skewness by computing the data sample moments ratio known as the \textit{Skewness} index which result to be `r round(e1071::skewness(rain$rain, type = 1), 2)`, agreeing with the histogram depicted.


<!-- introduce to temperature data -->
Afterwards, we investigate the temperature data. As for the rain data set, we have availability of the information from 1901 up to 2020. In figure \ref{fig:esa_temp_hist} we depict the annual temperature distribution. 

\begin{figure}
```{r eda_temp}
temp <- read.csv('../data/tas_1901_2020_ZAF.csv') # load temp data
names(temp)[1] <- 'temp' # assign name to data set
temp_mean <-tapply(temp$temp, temp$Year, mean) # compute average year temp
# assign to an handy data set
temp <- data.frame(year = names(temp_mean), temp = temp_mean)
# plot histogram
hist(temp$temp, xlab = 'temperature', main = 'Annual temperature distribution', 
     col = 'grey90', border = 'white', breaks = 10)
```
\label{fig:esa_temp_hist}
\caption{Annual temperature distribution}
\end{figure}

We can clearly observe a fairly strong right skeweness indicating more extreme events such as sweltering days than very cold days. The phenomenon now takes values in the real domain $x \in \mathbb{R}$ and present a skewness index of `r round(e1071::skewness(temp$temp, type = 1), 2)`.



<!-- move to lowess -->
In order to give track the evolution of the phenomenon over time and to give an idea of the overall trend we make use of non parametric model, meaning that we do not assume anything but a relation between $\mathbf{x}$ and $\mathbf{x}$ governed by a function $f(.)$. The method employed is the locally weighted regression discussed in section \ref{sec:techback}. 


Therefore, to track the evolution of the phenomenon we fit the model for different values of the span. In particular:

- $100\%$ of the neighbor data to take into account the global context (violet)

- $50\%$ of the neighbor data to allow for a more local structure (blue)

By investigating the results in \ref{fig:esa_rain} we can notice how the global fit to the rain data shows a constant trend up to the 1950 which evolves in a slightly linear decreasing trend onward. On the other hand, the local fit shows a constant trend up to the 1980s which evolves in a steeper decrease for the later year. Nevertheless, both the models suggest an evolution of the amount of rainfall which changes over time.

\begin{figure}
```{r eda_rainfall}
# plot non parametric local regression to the data of interest + graphics parameter

# span value: 1
plot_lowess(rain$year, rain$rain, f = 1, 
            main = 'Average rain fall amount per year', ax = rain$year,
            xlab = 'year', ylab = 'rain fall amount', type = 'l')

# span value: .5
plot_lowess(rain$year, rain$rain, f = .5, 
            main = 'Average rain fall amount per year', ax = rain$year,
            xlab = 'year', ylab = 'rain fall amount', type = 'l',
            add = 1, fit.col = 'royalblue')

# add legend
legend('topright', legend = c('span 100%', 'span  50%'), lty = 1, lwd = 2,
       bty = 'n', col = c('violetred', 'royalblue'))
```
\label{fig:esa_rain}
\caption{Annual rainfall}
\end{figure}


Afterwards, we fit a the same two models for the temperature data. Here the relation with time is much clearer. For instance, we can observe how the global fit shows a linearly increasing trend over time, with constant velocity. On the other hand, the local fit shows an increasing trend with different intensities in different period allowing for non linear relationship. Nevertheless, the overall trend does not change dramatically. Annual temperature has clearly been increasing over the last century.

```{r eda_temp_fit}
# fit local regression and produce plot of the result


# span 100%
plot_lowess(temp$year, temp$temp, f = 1, ax = temp$year,
            xlab = 'year', ylab = 'temperature', type = 'l',
            main = 'Annual temperature over year')

# span 50%
plot_lowess(temp$year, temp$temp, f = .5, main = 'yearly temp', ax = temp$year,
            xlab = 'year', ylab = 'temperature', type = 'l',
            add = 1, fit.col = 'blue')

# add legend
legend('topleft', legend = c('span 100%', 'span  50%'), lty = 1, lwd = 2,
       bty = 'n', col = c('violetred', 'royalblue'))
```
```{r esa_loadX}
X <- merge(rain, temp, by = 'year') # join data in a dataset by year 
```

\begin{table}[ht]
\centering
\begin{tabular}{rrrr}
  \hline
 & year & rain & temp \\ 
  \hline
year & 1.00 & -0.06 & 0.82 \\ 
  rain & -0.06 & 1.00 & -0.33 \\ 
  temp & 0.82 & -0.33 & 1.00 \\ 
   \hline
\end{tabular}
\label{esa:cor_matrix}
\caption{Correlation table}
\end{table}


<!-- pairs plot and correlation -->
In order to give a global idea of the data we are investigating we would like to present the pairs plot showing each pair combination plus the estimated correlation matrix $\mathbf{R}$ in table \ref{esa:cor_matrix}. As mentioned earlier, in figure \ref{fig:esa_pairs} we can notice the strong relationship between temperature and year $\hat{\rho}_{t,y} =$ `r round(cor(X[,-2])[2], 2)` and a weaker relationship between rain and year $\hat{\rho}_{r,y} =$ `r round(cor(X[,-3])[2], 2)` . However, we can see a clear relationship between year and rain $\hat{\rho}_{r,t}=$ `r round(cor(X[,-1])[2], 2)` which may be worth further investigation.

\begin{figure}
```{r esa_pairs_plot, fig.width= 3, fig.height=3}
par(mar = rep(0, 4))
X <- merge(rain, temp, by = 'year') # join data in a dataset by year 
# pairs(X, col = 'grey60') # shows pairs plot to give an idea of the relationship
plot(X[-3], xlab = 'year', ylab = 'rain', col = 'grey60', xaxt = 'n')
plot(X[-1], xlab = 'rain', ylab = 'temp', col = 'grey60', xaxt = 'n')
plot(X[-2], xlab = 'year', ylab = 'temp', col = 'grey60', xaxt = 'n')
# cor(X) # output correlation table
# xtable(cor(X), type = "latex") # output table for latex
```
\label{fig:esa_pairs}
\caption{Year, Rain, Temperature scatterplot}
\end{figure}


<!-- ------------------------------------------------------------------------------------------------------------ -->
\section{Climate Modeling}
\label{sec:climate}

We are now going to propose a very simple physical model to describe the temperature behavior. Ascertained that temperature heavily evolves over time, we present a very simple ordinary different equation given the temperature $\tau$ and time $t$

$$ \frac{d}{d t}\ \tau = \gamma\ \tau $$
which posit a linear evolution of the phenomenon over time. Moreover, we add initial condition equal to the average temperature values of the first 5 year. The reason behind this choice is that, within this project, we do not have access to older data and during the exploratory data analysis in section ref{sec:eda} we have seen how the phenomenon is highly variable. Therefore, we choose the mean as initial condition
$$f(\tau, t = 0) = \tau_0 =16.8 $$

Then we need to tune the parameter $\gamma$ and we are going to do so exploiting the Levenberg-Marquardt algorithm discussed in section \ref{sec:techback}.


```{r climate_model_temp_ode}
# model temperature in time with gamma parameter
tempchange <- function(t, state, parms) {
  
  with(as.list(c(state,parms)), {
    
    gamma <- as.numeric(parms["gamma"]) # gamma parameter
    dtemp <- gamma * temp  # temperature evolution dxdt = gamma x 
    
    return(list(c(dtemp))) # return object as a list
  })
}

# numerically integrate ODE
t <- X$time <- X$year - min(X$year, na.rm = TRUE) # extract time from t = 0 for ODE
yini <- c(temp = mean(X$temp[1:5])) # assign initial condition as the average of the first 5y
parms <- c(gamma = 1e-3) # give initial guess for the parametrs

# function that calculates residual sum of squares for LM algorithm
ssq_temp <- function(parms){
  
  # inital temperature as 5y average
  cinit <- c(temp = mean(X$temp[1:5]))
  
  # time points for which temperature is reported
  t <- sort( unique( c(seq(0,5,0.1), t ) ))
  
  # solve ODE for a given set of parameters
  out=ode(y=cinit,times=t,func=tempchange,parms=as.list(parms))
  
  # Filter data that contains time points where data is available
  outdf=data.frame(out)
  outdf=outdf[outdf$time %in% X$time,]
  # Evaluate predicted vs experimental residual
  ssqres <- outdf$temp - X$temp
  
  return(ssqres) # return predicted vs experimental residual
}


# fitting LM algorithm to the residual sum of square returned by the function ssq_temp
fitval_temp <- nls.lm(par = parms, fn = ssq_temp)
fit_pe <- fitval_temp$par
se <- summary(fitval_temp)$coefficients[1, 'Std. Error']
parest_temp <- c(fitval_temp$par - qnorm(.975) * se,
                 fitval_temp$par,
                 fitval_temp$par + qnorm(.975) * se)
```
The algorithm applied to the data set of interest returns a point estimate of $\hat{\gamma}=$`r round(parest_temp[2], 8)` and an estimated standard error of $\hat{SE}(\hat{\gamma})=$`r round(se, 8)`, resulting in the $95\%$ confidence interval

\begin{equation}
P(\gamma \in [0.00071764, 0.00081291]) = 0.95
\label{eq:gamma_ci}
\end{equation}

Overall, we can claim that, according to the model of interest, the global temperature increases over time by a factor within the region in (\ref{eq:gamma_ci}). We graphically present the estimated model in figure \ref{fig:temp_ode_evolution}. From the plot we can easily notice how the mean temperature increases over time. It also very noticeable how the model may underfit the data. It could be worth trying to fit a more complex model including higher order terms to address a certain amount of non-linearity. However, while these models may outperform the current in the data representation, they may be very unstable when employed in extrapolation task. The final goal here is to produce prediction about the future and a non-linear model may lead to very erratic conclusion. Therefore, we decide that this simple model fairly represents the phenomenon of interest.

\begin{figure}
```{r ode_temp_low_upp}
# estimate scenarios with point estimate
out_temp <- ode(y=yini,times=t,func=tempchange,parms=as.list(parest_temp[2]))
out_temp_l <- ode(y=yini,times=t,func=tempchange,parms=as.list(parest_temp[1]))
out_temp_u <- ode(y=yini,times=t,func=tempchange,parms=as.list(parest_temp[3]))

# plot point estimate
ylim <- range(X$temp)
plot(out_temp, ylim = ylim, main = 'Annual temperature ODE evolution',
     xlab = 'year', ylab = 'temperature', lwd = 2, col = 'violetred')
points(X$year - min(X$year), X$temp, col = 'grey70', type = 'l')
polygon(c(out_temp_l[,1], rev(out_temp_u[,1])),
        c(out_temp_l[,2], rev(out_temp_u[,2])), 
        border = NA, col = ggplot2::alpha('red', .1)
)
legend('topleft', col = c('violetred',ggplot2::alpha('red', .1)),
       lty = 1, lwd = c(2, 10), 
       legend = c('ODE estimation', '95% CI'), bty = 'n')
```
\caption{Annual temperature modeled by Ordinary Differential Equation}
\label{fig:temp_ode_evolution}
\end{figure}

We have now available a model which may lead us to future prediction. In particular, we are interested in what would happen in a 50 years time following the current trend. According to the estimated model and depicted in figure \ref{fig:temp_prediction}, the global temperature may continue to rise up to reaching $19$ Celcius average degree in 50 years time, with a reasonably tight confidence interval.

\begin{figure}
```{r temp_long_data}
# propose longer data for future prediction
t_long <- 0:170
X <- merge(X, data.frame(time = t_long), all.y = TRUE) # join data with future years
out_temp_long <- ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp))
out_temp_long <- data.frame(out_temp_long)

# predict longer data with lower, upper bound + point estimate
out_temp_lb = ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp[1]))
out_temp_pe = ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp[2]))
out_temp_ub = ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp[3]))
out_temp_par <- merge(merge(out_temp_lb, out_temp_pe, by = 'time'), out_temp_ub, by = 'time')
names(out_temp_par) <- c('time', 'lb', 'pe', 'ub') # assign name to data set 

# subset only out of sample year, those to be predicted
out_temp_par_only <- out_temp_par[out_temp_par$time > max(out_temp[,'time']),]

# plot result
par(mfrow = c(1,1), las = 2)
ylim <- range(out_temp_par[,-1], X$temp, na.rm = TRUE)
plot(t_long, X$temp, type = 'l', ylim = ylim, xaxt = 'n',
     xlab = 'year', ylab = 'temperature', col = 'grey70')
lines(out_temp_par$time, out_temp_par$pe, col = 'red')
polygon(c(out_temp_par_only$time, rev(out_temp_par_only$time)),
        c(out_temp_par_only$lb, rev(out_temp_par_only$ub)),
        col = ggplot2::alpha('red', .2), border = NA)
axis(1, at = X$time, labels = X$time + min(X$year, na.rm = 1), tick = FALSE)
legend('topleft', col = c('violetred',ggplot2::alpha('red', .1)),
       lty = 1, lwd = c(2, 10), 
       legend = c('ODE estimation', '95% CI'), bty = 'n')
```

\caption{50 out of sample years temperature prediction}
\label{fig:temp_prediction}
\end{figure}


<!-- local sensitivity analysis -->
A fundamental requisite for a mathematical model is the robustness to its input. Here, the data is empirically observed and the parameter $\gamma$ is estimated from it. The question we want to address is whether we can rely on this parameter and whether the model output would change if the parameters in input changed. In order to tackle this question, a common and reliable choice is local sensitivity analysis discussed in section \ref{sec:techback}. Given the model $$f(\tau) = \gamma\ \tau $$ the method allow us to derive the sensitivity equation as 

\begin{equation}
\begin{split}
\frac{d}{dt} s & = \frac{d}{d\gamma} f\  s + \frac{d}{d \gamma} f  \\
& = \gamma s + \tau
\end{split}
\end{equation}

Therefore, we will need to deal with the system of ordinary differential equation

\begin{equation}
\begin{cases}
\frac{d}{dt}\tau = \gamma \tau \\
\frac{d}{dt}s = \gamma s + \tau 
\end{cases}
\end{equation}



\begin{figure}
```{r temp_lsa}
tempsens <- function(t, state, parms) {
  
  with(as.list(c(state,parms)), {
    
    gamma <- as.numeric(parms["gamma"])
    RHS1 <- gamma * temp 
    RHS2 <- gamma * s + temp # derive sensitivity equation
    
    return(list(c(RHS2, RHS2)))
  })
}


# sensitivity: being only one parameter LSA is OK
# very solid estimation up to 100 SE

# increase qs if you want to investigate more extreme scenario

qs <- 10 ** (1:1) 
for(q in qs){
  
  sens <- c(yini, s = 0)
  pe0 <- parest_temp[2]
  pe1 <- pe0 + q * se
  pe2 <- pe0 - q * se
  out_temp_sens1 <- ode(y=sens,times=t,func=tempsens,parms=list(gamma = pe0))
  out_temp_sens2 <- ode(y=sens,times=t,func=tempsens,parms=list(gamma = pe1))
  out_temp_sens3 <- ode(y=sens,times=t,func=tempsens,parms=list(gamma = pe2))
  
  f <- function(x) log(x)
  ylim <- f(range(out_temp_sens1[,2], out_temp_sens2[,2], out_temp_sens3[,2]))
  plot(out_temp_sens1[,1],  f(out_temp_sens1[,2]), type = 'l', xlab = 'time',
       ylab = '', ylim = ylim)
  lines(out_temp_sens2[,1], f(out_temp_sens2[,2]), type = 'l', col = 'red')
  lines(out_temp_sens3[,1], f(out_temp_sens3[,2]), type = 'l', col = 'blue')
  
  ylim <- f(range(out_temp_sens1[,3], out_temp_sens2[,3], out_temp_sens3[,3]) + 1)
  plot(out_temp_sens1[,1],  f(out_temp_sens1[,3]),
       ylab = '', 
       xlab = 'time', type = 'l', ylim = ylim)
  lines(out_temp_sens2[,1], f(out_temp_sens2[,3]), col  = 'red')
  lines(out_temp_sens3[,1], f(out_temp_sens3[,3]), col  = 'blue')
}
```
\caption{a) evolution of temperature in time with input parameter $\pm$ 10 $SE$; b) evolution of $\gamma$ parameter in time $pm$ 10 $SE$}
\label{fig:temp_sens}
\end{figure}






<!-- ------------------------------------------------------------------------------------------------------------ -->
\section{Species count Modelling}
\label{sec:population}









<!-- ------------------------------------------------------------------------------------------------------------ -->
\section{Conclusion}
\label{sec:conclusion}



















<!-- ------------------------------------------------------------------------------------------------------------ -->
\bibliography{sample.bib} <!-- print references -->
\bibliographystyle{ieeetr} <!-- bibl style -->
