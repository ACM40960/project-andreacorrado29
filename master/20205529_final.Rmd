---
title: "South Africa drought and wildlife survival"
author: "Andrea Corrado 20205529"
bibliography: sample.bib
biblio-style: apalike
natbiboptions: round
header-includes:
  - \usepackage{float}
  - \usepackage[hypcap=true]{caption}
  - \usepackage{amsmath}
  - \usepackage{multicol}
output: bookdown::pdf_document2
---

```{r setup, include=FALSE, warning=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE, warning = FALSE, message = FALSE,
                      fig.align = 'center', dpi = 600)

rm(list = ls()) # clean env
gc() # clean memory

source('functions.R') # load useful functions  
source('pacakges_install.R') # install libraries if not available

# load useful libraries
library(ggplot2) 
library(reshape2) 
library(deSolve) 
library(minpack.lm)

```


\tableofcontents
\clearpage
\begin{multicols}{2}

\section*{Abstract}










\section{Introduction}

During last decades, interest and awareness about the climate change has steeply increased. There have been several studies examining and reporting the potential consequences of these phenomenon. For instance, the temperature and rainfall change has been of particular interest. Since last century, we have witnessed a increasing frequency in the number and severity of droughts in particular territories such as South Africa \textit{SA} \cite{SA_drought_decades}. The consequences here can be very dramatic. Due to the fact that area such as \textit{SA} suffer from low economic power, a drought may drive towards dramatic consequences It can affect either agriculture, for a period that lasts up to 6 months, either hydrological up to 24 months \cite{SA_drought_decades}. 

There have been several studies showing how to reduce the impact of drought on the agriculture field by growing restraint to higher temperature and computational model to predict droughts \cite{drought_asgriculture}.

However, in this paper, we wish to focus our attention on the drought consequences on wildlife population evolution. For instance, different studies have shown as animals mortality grows during the highest temperature period within the driest areas \cite{drought_wildlife_mortality_1990}. We will expand this idea to more recent data about waterbirds \cite{waterbirds_data}. This data set will provide the number of individual for different species in each year within the last 45 years. By doing so, we are able to track the evolution of the species during time. However, the data set will not provide the number of new death, new born and the relative causes. It will be our job to build a model which allows us to investigate the relation other factors. In particular the data we are going to analyse regard the global temperature and rain fall amount \cite{rain_temp_data} from 1901 up to 2020 which would give a foundation to build on the model of interest.

Having access to this information, our goal is to build a model which relates the climate change to the number of individual for each species and predict how, following the current trend, these would evolve over time and the potential consequences these could drive towards.


In section \ref{sec:techback} we give a brief introduction to the methodology used within this research project. In section \ref{sec:eda} we present the data with an exploratory data analysis. In section \ref{sec:climate} we model the climate data and provide prediction on a possible future scenario. In section \ref{sec:population} we model the species count in relation to the climate data and provide prediction about the future number of individuals. In section \ref{sec:conclusion} we discuss the finding of the project.


\section{Technical Background}
\label{sec:techback}

\subsection{Generalized Lienar Model}

In order to achieve the goal, we are going to employ different methodology which would allow us to model the data of interest. In particular, first of all we will related the temperature data to the rain fall amount. In order to do so, we will employ the Generalized Linear Model \cite{glm_intro} which theory would allow to establish a linear relationship between the quantity of interest (rain) and a set of covariates (temperature) plush a stochastic term which models the uncertainty about the random variable realisation

$$g(\mathbf{Y})  = \mathbf{X \beta} + \mathbf{\epsilon} $$
where we define a distribution $\mathcal{D}$ of interest on the stochastic term $\epsilon$
$$ \epsilon \sim \mathcal{D}(\theta)$$
and a link function $g(.)$ which is distribution dependent. 

The model will be validate by a set of diagnostic on the residuals $\mathbf{Y} - \mathbf{\hat{Y}}$ which provide meaningful insight on the goodness of fitness.

\subsection{Generalized Additive Model}

We will further extend the \textit{GLM} \cite{glm_intro} theory to allow for non-linear relationship between the predictors $\mathbf{X}$ and the response $\mathbf{Y}$ exploiting the \textit{Generalized Additive Model} theory \cite{gam_intro} while maintining the model linear in the parameters

$$g(\mathbf{Y})  = f(\mathbf{X}) + \mathbf{\epsilon} $$
where we define a distribution $\mathcal{D}$ of interest on the stochastic term $\epsilon$
$$ \epsilon \sim \mathcal{D}(\theta)$$
and a link function $g(.)$ which is distribution dependent. Moreover, we will define a function $f(.)$ which usually is a non linear function such as a \textit{spline} \cite{spline_intro}.


\subsection{Weighted Least Square}
During the research project, we perform different model estimation and to remedy to inadequate diagnostics, such as those violating the homoskedasticity assumption, we will employ \textit{Weighted Least Square} estimation theory \cite{wls_intro} which allows us to iteratively estimate a weight matrix $\mathbf{W}$ to be employed in the model estimation as a measure of the importance for each unit $\mathbf{x}_i\ i = \{1, \dots, N\}$ where $N$ is the total number of observations.

\subsection{Local Regression}



\subsection{Ordinary Differential Equation}
To track the evolution of a phenomenon over time, it is natural to think about \textit{Oridnary Differential Equation (ODE)} theory \cite{ode} which allows us to keep track of the change of a quantity in continuous time $$ \frac{d}{dt} x = f(x) $$ given initial condition $f(x, t = 0) = x_0$

\subsection{Levenberg-Marquardt algorithm}
Very often, the function defined within an ODE is characterized by a set of parameters $\mathbf{\beta}$. In empirical studies,we often have access to data realization $\{(y_i, \mathbf{x}_i, )\}_{i=1}^n$ we want to find the parameters $\mathbf{\beta}$ so that $y = f(\mathbf{x}, \mathbf{\beta})$ best fits the data of interest.  The \textit{Levenberg-Marquardt} \cite{lm_algo} provide a useful and efficient solution to the problem by making smart combination of \textit{Gauss-Newton} and \textit{Gradien Descend} theory.

\subsection{Sensitivity Analysis}
Mathematical modeling is often subject to very strong assumption which may be difficult to evaluate. In order to provide a robust framework, it is necessary to validate the model under perturbations on the findings. This is why, we will exploit \textit{Local Sensitivity Analysis} theory \cite{lsa_intro} to evaluate the robustness of the estimated models.


\section{Exploratory Data Analysis}
\label{sec:eda}

\subsection{Rainfall amount}

Within this exploratory data analysis context, we give a short introduction to the data by illustrating some characteristics. In the first place, we investigate the rainfall data set. We have access to the information between January 1901 up to December 2020. For simplicity, we decide to compute an annual average and inspect the resulting distribution.

\begin{figure}[H]
```{r esa_rain_preview}
rain <- read.csv('../data/pr_1901_2020_ZAF.csv') # load rain data
names(rain)[1] <- 'rain' # assign name to data set
rain_mean <- tapply(rain$rain, rain$Year, mean) # compute annual mean
rain_sd <- tapply(rain$rain, rain$Year, sd) # compute annual standard deviation
# collect data in a easy to handle data set
rain <- data.frame(year = as.numeric(names(rain_mean)), rain = rain_mean)

# plot histogram
hist(rain$rain, xlab = 'rain', main = 'Annual rain distribution', 
     col = 'grey90', border = 'white', breaks = 10)
```
\caption{Yearly rainfall distribution}
\label{fig:esa_rain_hist}
\end{figure}

From figure \ref{fig:esa_rain_hist} we can observe that the data distribution is somewhat bell-shaped. However, we can easily notice that the distribution is right skewed.  Clearly, the phenomenon is left bounded by 0 $x \in \mathbb{R}_+$ and we can rarely observe very extreme event such as very heavy precipitation. We are able to quantify the amount of skewness by computing the data sample moments ratio known as the \textit{Skewness} index which result to be `r round(e1071::skewness(rain$rain, type = 1), 2)`, agreeing with the histogram depicted.


Afterwards, we investigate the temperature data. As for the rain data set, we have availability of the information from 1901 up to 2020. In figure \ref{fig:esa_temp_hist} we depict the annual temperature distribution. 

\begin{figure}[H]
```{r eda_temp}
temp <- read.csv('../data/tas_1901_2020_ZAF.csv') # load temp data
names(temp)[1] <- 'temp' # assign name to data set
temp_mean <-tapply(temp$temp, temp$Year, mean) # compute average year temp
# assign to an handy data set
temp <- data.frame(year = names(temp_mean), temp = temp_mean)
# plot histogram
hist(temp$temp, xlab = 'temperature', main = 'Annual temperature distribution', 
     col = 'grey90', border = 'white', breaks = 10)
```
\label{fig:esa_temp_hist}
\caption{Annual temperature distribution}
\end{figure}

We can clearly observe a fairly strong right skeweness indicating more extreme events such as sweltering days than very cold days. The phenomenon now takes values in the real domain $x \in \mathbb{R}$ and present a skewness index of `r round(e1071::skewness(temp$temp, type = 1), 2)`.



In order to give track the evolution of the phenomenon over time and to give an idea of the overall trend we make use of non parametric model, meaning that we do not assume anything but a relation between $\mathbf{x}$ and $\mathbf{x}$ governed by a function $f(.)$. The method employed is the locally weighted regression discussed in section \ref{sec:techback}. 


Therefore, to track the evolution of the phenomenon we fit the model for different values of the span. In particular:

- $100\%$ of the neighbor data to take into account the global context (violet)

- $50\%$ of the neighbor data to allow for a more local structure (blue)

By investigating the results in \ref{fig:esa_rain} we can notice how the global fit to the rain data shows a constant trend up to the 1950 which evolves in a slightly linear decreasing trend onward. On the other hand, the local fit shows a constant trend up to the 1980s which evolves in a steeper decrease for the later year. Nevertheless, both the models suggest an evolution of the amount of rainfall which changes over time.

\begin{figure}[H]
```{r eda_rainfall}
# plot non parametric local regression to the data of interest + graphics parameter

# span value: 1
plot_lowess(rain$year, rain$rain, f = 1, 
            main = 'Average rain fall amount per year', ax = rain$year,
            xlab = 'year', ylab = 'rain fall amount', type = 'l')

# span value: .5
plot_lowess(rain$year, rain$rain, f = .5, 
            main = 'Average rain fall amount per year', ax = rain$year,
            xlab = 'year', ylab = 'rain fall amount', type = 'l',
            add = 1, fit.col = 'royalblue')

# add legend
legend('topright', legend = c('span 100%', 'span  50%'), lty = 1, lwd = 2,
       bty = 'n', col = c('violetred', 'royalblue'))
```
\label{fig:esa_rain}
\caption{Annual rainfall}
\end{figure}


Afterwards, we fit a the same two models for the temperature data. Here the relation with time is much clearer. For instance, we can observe how the global fit shows a linearly increasing trend over time, with constant velocity. On the other hand, the local fit shows an increasing trend with different intensities in different period allowing for non linear relationship. Nevertheless, the overall trend does not change dramatically. Annual temperature has clearly been increasing over the last century.

```{r eda_temp_fit}
# fit local regression and produce plot of the result


# span 100%
plot_lowess(temp$year, temp$temp, f = 1, ax = temp$year,
            xlab = 'year', ylab = 'temperature', type = 'l',
            main = 'Annual temperature over year')

# span 50%
plot_lowess(temp$year, temp$temp, f = .5, main = 'yearly temp', ax = temp$year,
            xlab = 'year', ylab = 'temperature', type = 'l',
            add = 1, fit.col = 'blue')

# add legend
legend('topleft', legend = c('span 100%', 'span  50%'), lty = 1, lwd = 2,
       bty = 'n', col = c('violetred', 'royalblue'))
```
```{r esa_loadX}
X <- merge(rain, temp, by = 'year') # join data in a dataset by year 
```

\begin{table}[h]
\centering
\begin{tabular}{rrrr}
  \hline
 & year & rain & temp \\ 
  \hline
year & 1.00 & -0.06 & 0.82 \\ 
  rain & -0.06 & 1.00 & -0.33 \\ 
  temp & 0.82 & -0.33 & 1.00 \\ 
   \hline
\end{tabular}
\label{esa:cor_matrix}
\caption{Correlation table}
\end{table}


In order to give a global idea of the data we are investigating we would like to present the pairs plot showing each pair combination plus the estimated correlation matrix $\mathbf{R}$ in table \ref{esa:cor_matrix}. As mentioned earlier, in figure \ref{fig:esa_pairs} we can notice the strong relationship between temperature and year $\hat{\rho}_{t,y} =$ `r round(cor(X[,-2])[2], 2)` and a weaker relationship between rain and year $\hat{\rho}_{r,y} =$ `r round(cor(X[,-3])[2], 2)` . However, we can see a clear relationship between year and rain $\hat{\rho}_{r,t}=$ `r round(cor(X[,-1])[2], 2)` which may be worth further investigation.
\end{multicols}

\begin{figure}[H]
```{r esa_pairs_plot}
par(mar = rep(0, 4))
X <- merge(rain, temp, by = 'year') # join data in a dataset by year 
pairs(X, col = 'grey60') # shows pairs plot to give an idea of the relationship
# plot(X[-3], xlab = 'year', ylab = 'rain', col = 'grey60', xaxt = 'n')
# plot(X[-1], xlab = 'rain', ylab = 'temp', col = 'grey60', xaxt = 'n')
# plot(X[-2], xlab = 'year', ylab = 'temp', col = 'grey60', xaxt = 'n')
# cor(X) # output correlation table
# xtable(cor(X), type = "latex") # output table for latex
```
\label{fig:esa_pairs}
\caption{Year, Rain, Temperature scatterplot}
\end{figure}

\begin{multicols}{2}

\section{Climate Modeling}
\label{sec:climate}

We are now going to propose a very simple physical model to describe the temperature behavior. Ascertained that temperature heavily evolves over time, we present a very simple ordinary different equation given the temperature $\tau$ and time $t$

$$ \frac{d}{d t}\ \tau = \gamma\ \tau $$
which posit a linear evolution of the phenomenon over time. Moreover, we add initial condition equal to the average temperature values of the first 5 year. The reason behind this choice is that, within this project, we do not have access to older data and during the exploratory data analysis in section ref{sec:eda} we have seen how the phenomenon is highly variable. Therefore, we choose the mean as initial condition
$$f(\tau, t = 0) = \tau_0 =16.8 $$

Then we need to tune the parameter $\gamma$ and we are going to do so exploiting the Levenberg-Marquardt algorithm discussed in section \ref{sec:techback}.


```{r climate_model_temp_ode}
# model temperature in time with gamma parameter
tempchange <- function(t, state, parms) {
  
  with(as.list(c(state,parms)), {
    
    gamma <- as.numeric(parms["gamma"]) # gamma parameter
    dtemp <- gamma * temp  # temperature evolution dxdt = gamma x 
    
    return(list(c(dtemp))) # return object as a list
  })
}

# numerically integrate ODE
t <- X$time <- X$year - min(X$year, na.rm = TRUE) # extract time from t = 0 for ODE
yini <- c(temp = mean(X$temp[1:5])) # assign initial condition as the average of the first 5y
parms <- c(gamma = 1e-3) # give initial guess for the parametrs

# function that calculates residual sum of squares for LM algorithm
ssq_temp <- function(parms){
  
  # inital temperature as 5y average
  cinit <- c(temp = mean(X$temp[1:5]))
  
  # time points for which temperature is reported
  t <- sort( unique( c(seq(0,5,0.1), t ) ))
  
  # solve ODE for a given set of parameters
  out=ode(y=cinit,times=t,func=tempchange,parms=as.list(parms))
  
  # Filter data that contains time points where data is available
  outdf=data.frame(out)
  outdf=outdf[outdf$time %in% X$time,]
  # Evaluate predicted vs experimental residual
  ssqres <- outdf$temp - X$temp
  
  return(ssqres) # return predicted vs experimental residual
}


# fitting LM algorithm to the residual sum of square returned by the function ssq_temp
fitval_temp <- nls.lm(par = parms, fn = ssq_temp)
fit_pe <- fitval_temp$par
se <- summary(fitval_temp)$coefficients[1, 'Std. Error']
parest_temp <- c(fitval_temp$par - qnorm(.975) * se,
                 fitval_temp$par,
                 fitval_temp$par + qnorm(.975) * se)
```
The algorithm applied to the data set of interest returns a point estimate of $\hat{\gamma}=$`r round(parest_temp[2], 8)` and an estimated standard error of $\hat{SE}(\hat{\gamma})=$`r round(se, 8)`, resulting in the $95\%$ confidence interval

\begin{equation}
P(\gamma \in [0.00071764, 0.00081291]) = 0.95
\label{eq:gamma_ci}
\end{equation}

Overall, we can claim that, according to the model of interest, the global temperature increases over time by a factor within the region in (\ref{eq:gamma_ci}). We graphically present the estimated model in figure \ref{fig:temp_ode_evolution}. From the plot we can easily notice how the mean temperature increases over time. It also very noticeable how the model may underfit the data. It could be worth trying to fit a more complex model including higher order terms to address a certain amount of non-linearity. However, while these models may outperform the current in the data representation, they may be very unstable when employed in extrapolation task. The final goal here is to produce prediction about the future and a non-linear model may lead to very erratic conclusion. Therefore, we decide that this simple model fairly represents the phenomenon of interest.

\begin{figure}[H]
```{r ode_temp_low_upp}
# estimate scenarios with point estimate
out_temp <- ode(y=yini,times=t,func=tempchange,parms=as.list(parest_temp[2]))
out_temp_l <- ode(y=yini,times=t,func=tempchange,parms=as.list(parest_temp[1]))
out_temp_u <- ode(y=yini,times=t,func=tempchange,parms=as.list(parest_temp[3]))

# plot point estimate
ylim <- range(X$temp)
plot(out_temp, ylim = ylim, main = 'Annual temperature ODE evolution',
     xlab = 'year', ylab = 'temperature', lwd = 2, col = 'violetred')
points(X$year - min(X$year), X$temp, col = 'grey70', type = 'l')
polygon(c(out_temp_l[,1], rev(out_temp_u[,1])),
        c(out_temp_l[,2], rev(out_temp_u[,2])), 
        border = NA, col = ggplot2::alpha('red', .1)
)
legend('topleft', col = c('violetred',ggplot2::alpha('red', .1)),
       lty = 1, lwd = c(2, 10), 
       legend = c('ODE estimation', '95% CI'), bty = 'n')
```
\caption{Annual temperature modeled by Ordinary Differential Equation}
\label{fig:temp_ode_evolution}
\end{figure}

We have now available a model which may lead us to future prediction. In particular, we are interested in what would happen in a 50 years time following the current trend. According to the estimated model and depicted in figure \ref{fig:temp_prediction}, the global temperature may continue to rise up to reaching $19$ Celcius average degree in 50 years time, with a reasonably tight confidence interval.

\begin{figure}[H]
```{r temp_long_data}
# propose longer data for future prediction
t_long <- 0:170
X <- merge(X, data.frame(time = t_long), all.y = TRUE) # join data with future years
out_temp_long <- ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp))
out_temp_long <- data.frame(out_temp_long)

# predict longer data with lower, upper bound + point estimate
out_temp_lb = ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp[1]))
out_temp_pe = ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp[2]))
out_temp_ub = ode(y=yini,times=t_long,func=tempchange,parms=as.list(parest_temp[3]))
out_temp_par <- merge(merge(out_temp_lb, out_temp_pe, by = 'time'), out_temp_ub, by = 'time')
names(out_temp_par) <- c('time', 'lb', 'pe', 'ub') # assign name to data set 

# subset only out of sample year, those to be predicted
out_temp_par_only <- out_temp_par[out_temp_par$time > max(out_temp[,'time']),]

# plot result
par(mfrow = c(1,1), las = 2)
ylim <- range(out_temp_par[,-1], X$temp, na.rm = TRUE)
plot(t_long, X$temp, type = 'l', ylim = ylim, xaxt = 'n',
     xlab = 'year', ylab = 'temperature', col = 'grey70')
lines(out_temp_par$time, out_temp_par$pe, col = 'red')
polygon(c(out_temp_par_only$time, rev(out_temp_par_only$time)),
        c(out_temp_par_only$lb, rev(out_temp_par_only$ub)),
        col = ggplot2::alpha('red', .2), border = NA)
axis(1, at = X$time, labels = X$time + min(X$year, na.rm = 1), tick = FALSE)
legend('topleft', col = c('violetred',ggplot2::alpha('red', .1)),
       lty = 1, lwd = c(2, 10), 
       legend = c('ODE estimation', '95% CI'), bty = 'n')
```

\caption{50 out of sample years temperature prediction}
\label{fig:temp_prediction}
\end{figure}


A fundamental requisite for a mathematical model is the robustness to its input. Here, the data is empirically observed and the parameter $\gamma$ is estimated from it. The question we want to address is whether we can rely on this parameter and whether the model output would change if the parameters in input changed. In order to tackle this question, a common and reliable choice is local sensitivity analysis discussed in section \ref{sec:techback}. Given the model $$f(\tau) = \gamma\ \tau $$ the method allow us to derive the sensitivity equation as 

\begin{equation}
\begin{split}
\frac{d}{dt} s & = \frac{d}{d\gamma} f\  s + \frac{d}{d \gamma} f  \\
& = \gamma s + \tau
\end{split}
\end{equation}

Therefore, we will need to deal with the system of ordinary differential equation

\begin{equation}
\begin{cases}
\frac{d}{dt}\tau = \gamma \tau \\
\frac{d}{dt}s = \gamma s + \tau 
\end{cases}
\end{equation}

As done previously, we numerically integrate the system of \textit{ODE} and inspect the result. In particular, we want to investigate what would happen with different values for the input parameter $\gamma$. In this setting, we try to evaluate the model output for values which are 5 time the estimated standard error away from the point estimate $\gamma \pm 5 \times \hat{SE}(\hat{\gamma})$. In figure \ref{fig:temp_sens} we can observe the system output. In the first place, plotting the temperature values $\tau$ against time $t$ in log-scale, we do not observe any appreciable difference for such an extreme value of the parameter considered. In the second plot, we plot the sensitivity value $s$ against time $t$ and, as before, we cannot notice any significant difference. This analysis clearly state how the model of interest is robust to perturbations of the input.

\begin{figure}[H]
```{r temp_lsa}
tempsens <- function(t, state, parms) {
  
  with(as.list(c(state,parms)), {
    
    gamma <- as.numeric(parms["gamma"])
    RHS1 <- gamma * temp 
    RHS2 <- gamma * s + temp # derive sensitivity equation
    
    return(list(c(RHS2, RHS2)))
  })
}


# sensitivity: being only one parameter LSA is OK
# very solid estimation up to 50 SE

# increase qs if you want to investigate more extreme scenario
qs <- 5
for(q in qs){
  
  sens <- c(yini, s = 0)
  pe0 <- parest_temp[2]
  pe1 <- pe0 + q * se
  pe2 <- pe0 - q * se
  out_temp_sens1 <- ode(y=sens,times=t,func=tempsens,parms=list(gamma = pe0))
  out_temp_sens2 <- ode(y=sens,times=t,func=tempsens,parms=list(gamma = pe1))
  out_temp_sens3 <- ode(y=sens,times=t,func=tempsens,parms=list(gamma = pe2))
  
  f <- function(x) log(x)
  ylim <- f(range(out_temp_sens1[,2], out_temp_sens2[,2], out_temp_sens3[,2]))
  plot(out_temp_sens1[,1],  f(out_temp_sens1[,2]), type = 'l', xlab = 'time',
       ylab = '', ylim = ylim)
  lines(out_temp_sens2[,1], f(out_temp_sens2[,2]), type = 'l', col = 'red')
  lines(out_temp_sens3[,1], f(out_temp_sens3[,2]), type = 'l', col = 'blue')
  
  ylim <- f(range(out_temp_sens1[,3], out_temp_sens2[,3], out_temp_sens3[,3]) + 1)
  plot(out_temp_sens1[,1],  f(out_temp_sens1[,3]),
       ylab = '', 
       xlab = 'time', type = 'l', ylim = ylim)
  lines(out_temp_sens2[,1], f(out_temp_sens2[,3]), col  = 'red')
  lines(out_temp_sens3[,1], f(out_temp_sens3[,3]), col  = 'blue')
}
```
\caption{Evolution of temperature in time with input parameter $\pm$ 5 $\hat{SE}(\hat{\gamma})$; b) evolution of $\gamma$ parameter in time $\pm$ 5 $\hat{SE}(\hat{\gamma})$}
\label{fig:temp_sens}
\end{figure}

Once ascertained the robustness of the temperature model, we are ready to investigate between the latter and the rainfall amount. In order to do so, we estimate a linear regression model where we add a non-linear component derived by the spline theory which allows us to write 
$$ \textbf{rain} = \beta_0 + f(\mathbf{\tau}) + \mathbf{\epsilon}$$

The first trial is not satisfactory as the model present some deficiency. In particular, from the left plot in figure \ref{fig:res_fit_ols} we can observe the presence of heteroskedasticity, meaning that the variance is not constant over the range of values for the value in input. In order to address the problem, we perform an iterative weighted least square estimation gaining a $29\%$ reduction in the estimated $BIC$ values, as shown in figure\ref{fig:res_fit_wls}.

\begin{figure}[H]
```{r temp_rain_model_data}
# need to understand how temp evolves
# rain_temp <- merge(rain, temp, by = 'year')
rain_temp <- X[complete.cases(X),]

# linear model
fit_rain <- lm(rain ~ splines::ns(temp, 2), data = rain_temp) # fit model
# BIC(fit_rain)
# summary(fit_rain)
plot(fitted(fit_rain), residuals(fit_rain), main = 'OLS',
     xlab = 'fitted', ylab = 'residuals', col = 'grey70')
legend('topleft', legend = paste('BIC', round(BIC(fit_rain),2)), bty = 'n')
```
\label{fig:res_fit_ols}
\caption{Residual vs Fitted plot for Ordinary Least Square model estimattion}
\end{figure}


\begin{figure}[H]
```{r rain_temp_wls}
fit_rain_wi <- fit_rain
for(i in 1:5){
  wi <- 1 / residuals(fit_rain_wi)**2
  fit_rain_wi <- lm(rain ~ splines::ns(temp,2), data = rain_temp, weights = wi)
}
plot(fitted(fit_rain_wi), residuals(fit_rain_wi), main = 'WLS',
     xlab = 'fitted', ylab = 'residuals', col = 'grey70')
legend('topleft', legend = paste('BIC', round(BIC(fit_rain_wi),2)), bty = 'n')
```
\label{fig:res_fit_wls}
\caption{Residual vs Fitted plot for Weighted Least Square model estimattion}
\end{figure}

The model parameters results are summarised in table \ref{tab:wls_model_summary}. Unsurprisingly, from those we can observe how both the estimated spline coefficients have negative sign with very tiny estimated standard error. Therefore, we can appreciate how, up to 8 decimal precision, the returned P-value is $0$, meaning that we reject the null hypothesis $H_0:\ beta_i = 0$ for $i = \{0, 1, 2\}$. This means that, according to the estimated model, the relation between temperature and rainfall amount is negative. With higher temperature we can expect to observe lower amount of rainfall and vice versa.

\end{multicols}
\begin{table}[H]
\centering
\begin{tabular}{rrrrr}
  \hline\
 & Estimate & Std. Error & t value & Pr($>|t|$) \\ 
  \hline
$\hat{\beta}_0$ & 41.23907927 & 0.00008380 & 492090.20447958 & 0.00000000 \\ 
  $\hat{\beta}_1$ & -7.90378231 & 0.00015488 & -51030.55292079 & 0.00000000 \\ 
  $\hat{\beta}_2$ & -8.53033020 & 0.00001818 & -469180.40392581 & 0.00000000 \\ 
   \hline
\end{tabular}
\caption{Weighted Least Square model estimatio for temperature and rainfall amount}
\end{table}
\begin{multicols}{2}


We now need to evaluate the robustness of the model with respect to the parameters in input. As we did previously we will try two now configuration where we shift the parameters estimated by 5 times their standard deviation $\hat{\beta}_i \pm \hat{SE}(\hat{\beta}_i)$. The results are depicted in figure \ref{fig:rain_time_sens_5se}. Here can notice how the different scenario have different intensities but the overall trend is very similar leading to likewise trend in the long period.



\begin{figure}[H]
```{r rain_temp_prediction, warning=FALSE}
# predict rain from temp
pred_type <- 'prediction'
rain_pred_pe <- predict(fit_rain_wi, newdata = data.frame(temp = out_temp_par$pe), interval = pred_type)[,'fit']
rain_pred_lb <- predict(fit_rain_wi, newdata = data.frame(temp = out_temp_par$lb), interval = pred_type)[,'lwr']
rain_pred_ub <- predict(fit_rain_wi, newdata = data.frame(temp = out_temp_par$ub), interval = pred_type)[,'upr']

# sensitivity
# try to impute different values
q <- 5
fit_rain_wi_edit <- fit_rain_wi
fit_rain_wi_edit$coefficients[2] <-  -8.962673 + q *  1.0267
fit_rain_wi_edit$coefficients[3] <-  -8.6207 + q *  0.3338
pred_type <- 'prediction'
rain_pred_pe_e <- predict(fit_rain_wi_edit, newdata = data.frame(temp = out_temp_par$pe), interval = pred_type)[,'fit']
rain_pred_lb_e <- predict(fit_rain_wi_edit, newdata = data.frame(temp = out_temp_par$lb), interval = pred_type)[,'lwr']
rain_pred_ub_e <- predict(fit_rain_wi_edit, newdata = data.frame(temp = out_temp_par$ub), interval = pred_type)[,'upr']

fit_rain_wi_edit$coefficients[2] <-  -8.962673 - q *  1.0267
fit_rain_wi_edit$coefficients[3] <-  -8.6207 - q *  0.3338
pred_type <- 'prediction'
rain_pred_pe_f <- predict(fit_rain_wi_edit, newdata = data.frame(temp = out_temp_par$pe), interval = pred_type)[,'fit']
rain_pred_lb_f <- predict(fit_rain_wi_edit, newdata = data.frame(temp = out_temp_par$lb), interval = pred_type)[,'lwr']
rain_pred_ub_f <- predict(fit_rain_wi_edit, newdata = data.frame(temp = out_temp_par$ub), interval = pred_type)[,'upr']

par(las = 2)
plot(rain_temp$time, rain_temp$rain, type = 'l', xlim = range(out_temp_par$time),
     xaxt = 'n', col = 'grey70', xlab = 'temperature', ylab = 'rain')
axis(1, at = out_temp_par$time, labels = out_temp_par$time + min(X$year, na.rm = TRUE), tick = FALSE)
title('Rain vs Time')
lines(out_temp_par$time, rain_pred_pe, lwd = 2, col = 'red')
idx <- out_temp_par$time > max(rain_temp$time)
polygon(c(out_temp_par$time[idx], rev(out_temp_par$time[idx])),
        c(rain_pred_lb[idx], rev(rain_pred_ub[idx])),
        border = NA, col = ggplot2::alpha('red', .2))
# other scenario + se
lines(out_temp_par$time, rain_pred_pe_e, lwd = 2, col = 'blue')
polygon(c(out_temp_par$time[idx], rev(out_temp_par$time[idx])),
        c(rain_pred_lb_e[idx], rev(rain_pred_ub_e[idx])),
        border = NA, col = ggplot2::alpha('blue', .1))
# other scenario - se
lines(out_temp_par$time, rain_pred_pe_f, lwd = 2, col = 'green')
polygon(c(out_temp_par$time[idx], rev(out_temp_par$time[idx])),
        c(rain_pred_lb_f[idx], rev(rain_pred_ub_f[idx])),
        border = NA, col = ggplot2::alpha('green', .1))
legend('topright', col = c('red','blue','green'), bty = 'n', lty = 1, lwd = 2,
       legend = c('Estimated pars',
                  paste('est +', round(q, 2), 'SE'),
                  paste('est -', round(q, 2), 'SE')
                  )
)
```
\label{fig:rain_time_sens_5se}
\caption{Evolution of rainfall w.r.t. temperature with input parameter $\pm$ 5 $\hat{SE}(\hat{\gamma})$}
\end{figure}

************** write conclusion here ************************

\section{Species count Modelling}
\label{sec:population}

In the previous section we have presented the data and results on the climate data including temperature and rainfall information from 1901 up to 2021. In this section, we are going to present the waterbirds species count data over time and relation its evolution to the covariate explored earlier. In particular, the data set of interest provide the specie individual count per year from 1975 to 2021. Within this data set we observe monthly variation which, for simplicity and consistency with the climate data, will be averaged over the years. For different species, we have different number of observation and, as we observe up to 37 species, we decide to keep only those whose numerousness might provide robust result, from $n_i =30$ onward, as per figure \ref{fig:species_barplot}.


\begin{figure}[H]
```{r pop_data_viz}
rm(list = ls()) # clean env
source('functions.R') # load functions  

# laod packages: need to be installed if not available yet
suppressWarnings(library(ggplot2))
suppressWarnings(library(gam))
suppressWarnings(library(reshape2))
suppressWarnings(library(deSolve))
suppressWarnings(library(minpack.lm))
suppressWarnings(library(viridis))

# --------------------------------------------------------------------------- #
#                            load count data                                  #
# --------------------------------------------------------------------------- #

dat0 <- rbind(
  read.csv("../data/occurrence1.txt", row.names=1),
  read.csv("../data/occurrence2.txt", row.names=1),
  read.csv("../data/occurrence3.txt", row.names=1),
  read.csv("../data/occurrence4.txt", row.names=1)
)


dat <- dat0 # store data

# remove empty species
dat <- dat[ - which(dat$family == ''), ] 

# sanity check on year
dat <- dat[dat$year > 1900,]
years <- cbind( year = min(dat$year) : max(dat$year) )

key <- paste0(dat$year, dat$family) # creare compound key for YEAR : FAMILY
year_fam_count <- tapply(dat$family, key, length) # count yearly observation per family
fam_count <- substr(names(year_fam_count), 5, 999) # exclude year form result
fams_tab <- table(fam_count) # count how many year observation per species you have

# species count
par(las = 2)
barplot(fams_tab[order(fams_tab, decreasing = TRUE)], border = NA,
        col = ggplot2::alpha(viridis(length(fams_tab), direction = -1), .4) )
abline(h = 30, col = 'royalblue', lwd = 2, lty = 2)
```
\label{fig:species_barplot}
\caption{Years observation per species}
\end{figure}

Once selected the species of interest, we are interested in the overall trend. Beside a descriptive analysis, we provide a non-parametric fit by a local weighted regression to give an idea of the overall trend. As the values variance is appreaciable large, we provide either mean (red line) and median (blue line) value for robust result. Despite  being on different scale, both the results shows similar overall trend. The individual count has been decreasing over time. This behaviour is shown in figure \ref{fig:species_barplot}. 

\begin{figure}[H]
```{r y_wide}
# pre process data: create Y such that we will have the individual count
# for each species for each year (min #rows required -> 30)
Y <- data.frame()
min.row <- 30
fams <- unique(dat$family)
for(f in fams){
  i.dat <- dat[dat$family == f, ]
  i.years <- length(unique(i.dat$year))
  if(i.years >= min.row){ # if enough obs
    i.x <- tapply(i.dat$individualCount, i.dat$year, mean) # compute annual mean
    i.x <- cbind(year = names(i.x), individualCount = i.x, species = f) # data frame
    i.x <- merge(years, i.x, by = 'year', all.x = 1) # left join with years
    i.x$species[is.na(i.x$species)] <- f
    Y <- rbind(Y, i.x) # append to main data set 
  }
}

# create Y_wide such that we will have one species per column 
# and the related individual count per row
Y_wide <- tidyr::pivot_wider(Y,
                             names_from = 'species',
                             values_from = 'individualCount'
)

# we observe some "unusual spike" according to a population evolution
# we decide to remove it from the data
to_investigate <- Y[which.max(Y$individualCount), 'species']
# matplot(Y_wide[,1], Y_wide[,to_investigate], type = 'l')

# remove Glareolidae
Y <- Y[- which(Y$species == 'Glareolidae'),]
Y_wide <- Y_wide[, - which(colnames(Y_wide) == 'Glareolidae')]

# remove the NA cases
# compute annual mean
# fit a non parametric local regression to understand the trend
# quick viz
Y_complete <- Y[complete.cases(Y),]
Y_complete$individualCount <- as.numeric(Y_complete$individualCount)

means <- tapply(Y_complete$individualCount, Y_complete$year, mean)
medians <- tapply(Y_complete$individualCount, Y_complete$year, median)
years <- as.numeric(names(means))

# quick visualization
par(las = 2)
Y_wide <- data.frame(Y_wide)
matplot(Y_wide[,1], Y_wide[,-1], type = 'l', xaxt = 'n', lwd = .5, xlab = 'year', ylab = 'individual count')
lines(years, fitted(loess(means ~ years, span = 1)), lwd = 2, col = 'red')
lines(years, fitted(loess(medians ~ years, span = 1)), lwd = 2, col = 'blue')
axis(1, at = Y_wide[,1], labels = Y_wide[,1], tick = FALSE)
title('Species count vs time')
legend('topright', legend = c('Annual average', 'Annual median'),
       lty = 1, col = c('red', 'blue'), lwd = 2, bty = 'n')
```
\label{fig:species_barplot}
\caption{Yearly individual count per species plus mean and median observation}
\end{figure}

The next step is to relate the individual count data to the climate information. In the first place, we inspect the response distribution, as shown in figure \ref{fig:counts_hist}. As the data takes value on a discrete and positive set, it is reasonable to assume a \textit{Poisson} distribution.

\begin{figure}[H]
```{r count_hist}
# load climate data
X <- read.csv('../data/climate_pred.csv')[,-1]

# combine data ----------------------------------------------------------------

Y <- merge(Y, X, by = 'year', all.x = TRUE) # join data by year
Y <- Y[order(Y$species), ] # sort data by species
Y$individualCount <- round(
  as.numeric(Y$individualCount)
) # cast into num

# inspect individual count distribution: we can try a Pois distr
# par(mfrow = c(1,2))
#hist(Y$individualCount)
hist(Y$individualCount[Y$individualCount < 50], main= 'Count distribution',
     xlab = 'counts', col = ggplot2::alpha(viridis(10, direction = -1), .4),
     border = NA, freq = FALSE)
#par(mfrow = c(1,1))
```
\label{fig:counts_hist}
\caption{Distribution for animal counts}
\end{figure}

Afterwards, we are going to estimate a mixed-effect model 
*****************************

```{r glmer_model_fit}
# center data -----------------------------------------------------------------
cyear <- 1998
Y$syear <- Y$year - cyear

# fit model -------------------------------------------------------------------

# # define degree
# Q <- 6
# bics <- rep(NA, Q)
# for(q in 1:Q){
#   q_fit_year <- lme4::glmer(
#     individualCount ~ ns(syear, q) + (1 | species), 
#     family = 'poisson', data = Y
#   )
#   bics[q] <- BIC(q_fit_year)
# }
# # balance between model degree and interpretability
# q <- 4
# cols <- rep('royalblue', Q); cols[q] <- 'red'
# plot(1:Q, bics, type = 'b', col = cols, pch = 20)

# fit model
# fit_year <- lme4::glmer(
#   individualCount ~ ns(syear, q) + (1 | species), 
#   family = 'poisson', data = Y
# )


# what is the effect of year
# t <- sort(unique(Y$syear))
# t_pred <- ns(t, q) %*% as.numeric(coef(fit_year)[[1]][1,-1])
# plot(t, t_pred, type = 'l', main = 'effect of time')

# inspect residuals: we can see some structure not explained by syear
# therefore we will try include further predictors
# plot(fitted(fit_year), residuals(fit_year))

# further pred: preprocessing
m_rain <- mean(Y$rain_obs, na.rm = TRUE)
sd_rain <- sd(Y$rain_obs, na.rm = TRUE)
Y$srain <- (Y$rain_obs - m_rain)/sd_rain

m_temp <- mean(Y$temp_obs, na.rm = TRUE)
sd_temp <- sd(Y$temp_obs, na.rm = TRUE)
Y$stemp <- (Y$temp_obs - m_temp)/sd_temp


# Y$srain <- scale(Y$rain_obs, scale = TRUE)
# Y$stemp <- scale(Y$temp_obs, scale = TRUE)

# Q_temp <- 6
# bics <- rep(NA, Q_temp)
# for(q_temp in 1:Q_temp){
#   q_fit_year_temp <- lme4::glmer(
#     # this model returns the best AIC among those explored
#     individualCount ~ ns(syear, q_temp) * stemp + ns(syear, q_temp) * srain + (1 | species), 
#     family = 'poisson', data = Y
#   )
#   bics[q_temp] <- BIC(q_fit_year_temp)
# }
# 
# # balance between model degree and interpretability: again 4 may be a good choice
q_temp <- 4
# cols <- rep('royalblue', Q_temp); cols[q_temp] <- 'red'
# plot(1:Q_temp, bics, type = 'b', col = cols, pch = 20)

fit_year_temp <- lme4::glmer(
  # this model returns the best AIC among those explored
  individualCount ~ ns(syear, q_temp) * stemp + ns(syear, q_temp) * srain + (1 | species), 
  family = 'poisson', data = Y
)
```








\section{Conclusion}
\label{sec:conclusion}











\bibliography{sample.bib} 
\bibliographystyle{ieeetr} 










\end{multicols}

















